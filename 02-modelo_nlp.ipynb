{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de NLP\n",
    "El **Procesamiento del Lenguaje Natural** (más conocido como  NLP por su nombre en inglés, _Natural Language Processing_) es el área de estudio centrada en cómo los ordenadores entienden el lenguaje humano, lo interpretan y lo procesan. Se trata de un campo complejo en el que entran en juego diferentes disciplinas, entre las que podemos destacar la Inteligencia Artificial (AI), el _big data_ o la lingüistica.\n",
    "\n",
    "La mayor parte de las aplicaciones creadas dentro de este campo se enfocan en la comprensión, el manejo y la generación del lenguaje natural por parte de las máquinas. Entre ellas destacan:\n",
    "- Asistentes virtuales o chatbots.\n",
    "- Traducción automática de textos.\n",
    "- Clasificación de textos.\n",
    "- Resumen de textos.\n",
    "- Análisis de sentimientos.\n",
    "- ... y más!\n",
    "\n",
    "En este notebook utilizaremos el conjunto que hemos inspeccionado y adecuado para poder realizar un sencillo modelo que nos ayude a **analizar los sentimientos descritos en los diferentes tweets**. Así, el siguiente script está dividido en los siguientes bloques:\n",
    "- BLOQUE A: carga de datos inspeccionados.\n",
    "- BLOQUE B: preprocesamiento del texto.\n",
    "- BLOQUE C: representación del texto.\n",
    "- BLOQUE D: partición del conjunto de datos y balanceo.\n",
    "- BLOQUE E: entrenamiento del modelo de Gradient Boosting.\n",
    "- BLOQUE F: inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import gensim.downloader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOQUE A: Carga de datos\n",
    "Antes de comenzar, cargaremos los datos que han sido adecuados en nuestra fase anterior de limpieza y preprocesamiento de textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos ya adecuados\n",
    "df = pd.read_csv(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las primeras observaciones del conjunto\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOQUE B: Preprocesamiento del texto\n",
    "El preprocesamiento del texto es una fase importante dentro del Procesamiento del Lenguaje Natural (NLP). El objetivo de esta fase es la de transformar el texto en crudo, de manera que sea más fácilmente consumible por los algoritmos y modelos de Machine Learning (ML) y Deep Learning (DL) a aplicar.\n",
    "\n",
    "Esta fase consta de diferentes pasos y no son siempre los mismos. En este caso, preprocesaremos los teewts de la siguiente manera:\n",
    "\n",
    "1. **_Lower Casing_:** Transformar palabras de mayúsculas a minúsculas.\n",
    "\n",
    "2. **Reemplazar URLs:** Links que comienzan por \"http\" o \"https\" o \"www\" son reemplazados por la palabra \"URL\".\n",
    "\n",
    "3. **Reemplazar Emojis:** Reemplazar emojis usando un diccionario predefinido.\n",
    "\n",
    "4. **Reemplazar nombres de usuario:** Reemplazar @Nombres con la palabra \"USER\".\n",
    "\n",
    "5. **Eliminar _Non-Alphabets_:** Reemplazar todos los caracteres excepto dígitos and _alphabets_ por un espacio.\n",
    "\n",
    "6. **Eliminar letras consecutivas:** 3 o más letras consecutivas son reemplazadas por 2 letras (ejemplo: \"Heyyyy\" por \"Heyy\").\n",
    "\n",
    "7. **Eliminar palabras cortas:** Palabras con menos de 2 letras son eliminadas.\n",
    "\n",
    "8. **Eliminar _Stopwords_:** Las _Stopwords_ son aquellas palabras en ingés que no tienen un significado específico por si solas, por lo que pueden ser ignoradas sin sacrificar el significado de la oración (ejemplos: \"the\", \"he\").\n",
    "\n",
    "9. **_Stemming_:** Se refiere al proceso de eliminar sufijos y dar a la palabras una forma base, de modo que diferentes variaciones de una misma palabra puedan ser representadas en la misma forma (ejemplo: “walk” y “walking\" son ambas reducidas a \"walk\").\n",
    "\n",
    "10. **Tokenización:** Los modelos NLP normalmente analizan los textos dividiéndolos por palabras (_tokens_) y/o oraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario con los distions emojis y sus significados.\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para preprocesar el texto en crudo\n",
    "def preprocess(text):    \n",
    "    # Crear stemmer.\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    \n",
    "    # Crear lista de stopwords\n",
    "    en_stop = stopwords.words('english')\n",
    "    \n",
    "    # Definir patrones para reemplazar/eliminar.\n",
    "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern       = '@[^\\s]+'\n",
    "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
    "    sequencePattern   = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "\n",
    "    # Lower Casing\n",
    "    text = text.lower()\n",
    "    # Reemplazar URLs\n",
    "    text = re.sub(urlPattern,' URL', text)\n",
    "    # Reemplazar emojis.\n",
    "    for emoji in emojis.keys():\n",
    "        text = text.replace(emoji, \"EMOJI\" + emojis[emoji])        \n",
    "    # Reemplazar @Nombres con 'USER'.\n",
    "    text = re.sub(userPattern,' USER', text)        \n",
    "    # Reemplazar non-alphabets.\n",
    "    text = re.sub(alphaPattern, \" \", text)\n",
    "    # Reemplazar letras consecutivas.\n",
    "    text = re.sub(sequencePattern, seqReplacePattern, text)\n",
    "\n",
    "    # Tokenizar texto\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Eliminar palabras con menos de dos letras\n",
    "    tokens = [word for word in tokens if len(word)>2]\n",
    "    \n",
    "    # Eliminar stopwords\n",
    "    tokens = [word for word in tokens if word not in en_stop]\n",
    "    \n",
    "    # Aplicar stemmer o \"stemmizar\"\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "        \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la función a cada una de las filas del dataset\n",
    "df['preprocess_text'] = df[???].apply(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados del preprocesamiento: un ejemplo\n",
    "print('Texto en crudo:', df.loc[16, 'text'])\n",
    "print('Texto preprocesado:', df.loc[16, 'preprocess_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOQUE C: Representación del texto\n",
    "La conversión del texto en una representación númerica es uno de los pasos más importantes dentro de cualquier _pipeline_ de NLP. Esta conversión resulta esencial para que las \"máquinas\" puedan comprender y decodificar patrones dentro de cualquier lenguaje.\n",
    "\n",
    "Se trata de un proceso iterativo y que puede ser realizado mediante múltiples maneras o técnicas, abarcando desde las representaciones más sencillas (por ejemlo, _One hot encoding_) hasta otras más \"inteligentes\", que logran tener en cuenta las similitudes y diferencias entre ellas al basar su aprendizaje en redes neuronales (_Word embeddings_). En este [enlace](https://www.kaggle.com/code/nkitgupta/text-representations) podéis encontrar más información acerca de las diferentes técnicas normalmente empleadas.\n",
    "\n",
    "Nosotros utilizaremos esta última técnica, sirviéndonos de un algoritmo conocido como [GloVe](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo GloVe preentrenado\n",
    "GloveModel = gensim.downloader.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos ver la representación de una palabra\n",
    "GloveModel[???]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# También podemos ver las palabras con mayor similitud a otra\n",
    "GloveModel.most_similar(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construcción de nuestra matriz de representación\n",
    "\n",
    "# Función para obtener/calcular el vector de representación para cada tweet\n",
    "def get_w2v_vectors(processed_text, model = GloveModel):\n",
    "    # Guardamos el vocabulario del modelo Word2Vec en un objeto\n",
    "    words = model.index_to_key\n",
    "    \n",
    "    # Guardamos el tamaño de los vectores creados por el modelo en un objeto\n",
    "    size = model.vector_size\n",
    "    \n",
    "    # Iteramos sobre los tokens del tweet para obtener su vector en el modelo\n",
    "    text_vectors = []  # Lista vacía para poder guardar los vectores calculados\n",
    "\n",
    "    for token in processed_text:        \n",
    "        if token in words:\n",
    "            text_vectors.append(model[token])  # Si el token existe dentro del vocabulario, añadimos el valor de su vector\n",
    "            \n",
    "        else:\n",
    "            text_vectors.append(np.zeros(size))  # En caso de no existir, creamos un vector del mismo tamaño que sea todo 0's\n",
    "    \n",
    "    # Calculamos la media de todos los vectores de un tweet para poder crear una representación de todo el tweet\n",
    "    text_vectors_avg = np.mean(text_vectors, axis=0)\n",
    "            \n",
    "    return text_vectors_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la función a todo el conjunto de datos\n",
    "df['text_vector'] = df[???].apply(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOQUE D: Partición del conjunto de datos y balanceo del conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos nuestro dataset en dos conjuntos distintos: de entrenamiento y de test\n",
    "train_set, test_set = ????(df, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como habíamos visto en nuestra exploración del conjunto, sabemos que se encuentra claramente **desbalanceado** (más muestras de sentimiento negativo que de las otras dos clases). Si utilizaramos un conjunto desbalanceado para entrenar, nuestro modelo estaría claramente sesgado y le costaría aprender a diferenciar los patrones de las clases más minoritarias. Por tanto, procedemos a balancear el **conjunto de entrenamiento**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver distribución de las clases en el conjunto de entrenamiento\n",
    "train_set['airline_sentiment'].???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanceo del conjunto de entrenamiento\n",
    "train_neg = train_set[???]\n",
    "train_neutral = train_set[???]\n",
    "train_pos = train_set[???]\n",
    "\n",
    "num_minority = len(???)\n",
    "\n",
    "??? = resample(???, replace=False, n_samples=num_minority, random_state=0)\n",
    "??? = resample(???, replace=False, n_samples=num_minority, random_state=0)\n",
    "\n",
    "train_set_balanced = pd.???([train_neg, train_neutral, train_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos que el conjunto esta bien balanceado\n",
    "train_set_balanced['airline_sentiment'].???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por último, separamos nuestros sets de entrenamiento y test en conjuntos X e y\n",
    "X_train_balanced = list(train_set_balanced['???'])\n",
    "y_train_balanced = train_set_balanced['???']\n",
    "\n",
    "X_test = list(test_set['???'])\n",
    "y_test = test_set['???']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información acerca de los conjuntos\n",
    "print('Tamaño del conjunto de entrenamiento balanceado:', len(???))\n",
    "print('Tamaño del conjunto de test:', len(???))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOQUE E: Entrenamiento del modelo de Gradient Boosting\n",
    "¿Qué es _Boosting_?\n",
    "\n",
    "Boosting es un meta-algoritmo de aprendizaje automático que reduce el sesgo y la varianza en un contexto de aprendizaje supervisado. Consiste en combinar los resultados de varios clasificadores débiles para obtener un clasificador robusto. Cuando se añaden estos clasificadores débiles, se hace de modo que éstos tengan diferente peso en función de la exactitud de sus predicciones. Tras añadir un clasificador débil, los datos cambian su estructura de pesos: los casos mal clasificados ganan peso y los que son clasificados correctamente pierden peso.\n",
    "\n",
    "**Gradient Boosting (GB)** o Potenciación del gradiente consiste en plantear el problema como una optimización numérica en el que el objetivo es minimizar una función de coste añadiendo clasificadores débiles mediante el descenso del gradiente. Involucra tres elementos:\n",
    "\n",
    "- La **función de coste** a optimizar: depende del tipo de problema a resolver.\n",
    "- Un **clasificador débil** para hacer las predicciones: por lo general se usan árboles de decisión.\n",
    "- Un **modelo que añade (ensambla) los clasificadores débiles** para minimizar la función de coste: se usa el descenso del gradiente para minimizar el coste al añadir árboles.\n",
    "\n",
    "Los hiperparámetros más importantes que intervienen en este algoritmo (aunque no todos) son:\n",
    "- **learning_rate**: determina el impacto de cada árbol en la salida final. Se parte de una estimación inicial que se va actualizando con la salida de cada árbol. Es el parámetro que controla la magnitud de las actualizaciones.\n",
    "- **n_estimators**: número de clasificadores débiles a utilizar.\n",
    "\n",
    "Como en este caso utilizaremos **árboles de decisión** como clasificadores débiles a ensamblar, también debemos tener en cuenta los hiperparámetros que afectan a esta clase de modelos. En este caso:\n",
    "- **max_depth**: profundidad máxima del árbol.\n",
    "\n",
    "Más información sobre el modelo que se utiliza en este ejemplo y de sus parámetros [aquí](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo introduciendo los valores de los parámetros:\n",
    "gb_clf = ???(n_estimators=150, learning_rate=0.2, max_depth=3, random_state=0)\n",
    "\n",
    "# Entrenamiento o ajuste del modelo con los datos de entrenamiento\n",
    "gb_clf.fit(???, ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder ver como de bueno es nuestro modelo, podemos obtener las predicciones que realiza sobre los conjuntos de entrenamiento y test, y realizar el cálculo de alguna métrica para observar su rendimiento. En este caso, observaremos las métricas **precision**, **recall** y **F1 score**, las cuales son muy utilizadas sobre todo para casos en los que se trabaja con un alto desbalanceo de los datos.\n",
    "\n",
    "- **Precision** permite medir la calidad del modelo en tareas de clasificación. Para este ejemplo concreto, si nos centramos en los tweets postivos, mediría la cantidad de tweets positivos que nuestro modelo es capaz de identificar correctamente de entre todos los tweets que **nuestro modelo clasifica** como positivos. Para esta métrica, los falsos positivos son más importantes que los falsos negativos.\n",
    "\n",
    "- **Recall** informa sobre la cantidad que el modelo es capaz de identificar. Siguiendo con el ejemplo anterior, esta métrica nos permite cuantificar o medir la cantidad de tweets positivos que el modelo predice correctamente de entre todos los tweets que **realmente** son positivos. Al contratio que para _precision_, para esta métrica son más importantes los falsos negativos.\n",
    "\n",
    "- **F1 score** se utiliza para combinar ambas medidas, normalmente asumiendo que nos importan de igual forma.\n",
    "\n",
    "Más información detallada sobre estas métricas en este [enlace](https://mlu-explain.github.io/precision-recall/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre los datos de entrenamiento\n",
    "pred_train = gb_clf.predict(???)\n",
    "\n",
    "# Mostramos el \"classification report\"\n",
    "print('Resultados conjunto de entrenamiento:\\n')\n",
    "print(classification_report(y_train_balanced, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo sobre datos de test\n",
    "pred_test = gb_clf.predict(???)\n",
    "\n",
    "# Mostramos el \"classification report\"\n",
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOQUE F: Inferencia \n",
    "Una vez obtenido nuestro modelo final podemos utilizarlo para realizar inferencia sobre nuevos tweets no vistos anteriormente y catalogarlos así como positivos, neutros o negativos. Debemos tener en cuenta que los tweets que vayan a ser analizados mediante este modelo NLP deben someterse al mismo preprocesamiento y representación al que se han sometido el resto de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuevos tweets a clasificar\n",
    "new_tweets = [\"Don't travel with @UnitedAirlines_ ! They lost my luggage 2 months ago on a flight to Vegas, after 5 days they asked me to fill a claim because\" \\\n",
    "              \"they didn't know where my luggage was and specify what I had spent with receipts.\",\n",
    "              \"Virgin Atlantic and LATAM Airlines have submitted an application to the US Department of Transportation for a codeshare agreement which will create good \" \\\n",
    "              \"connectivity into three South American countries.\",\n",
    "              \"A wonderful flight to Paris on @airfrance done and dusted. Now for a short flight to Geneva on a beaut of a plane. I love the touch of the phone/iPad holders. \" \\\n",
    "              \"@Club_Med_SA #clubmedtignes #AirFranceZA\",\n",
    "              \"Very nice trip back from HEL with the brand new @AirFranceFR A220-330 F-HZUN from on AF1177. As usual excellent experience on board with the efficient\" \\\n",
    "              \"crew and a wonderful sunrise ! #Travel #Aircraft #a220 #airfrance #like #happy Good day everyone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamos los nuevos tweets para limpiar el texto\n",
    "preprocess_new_tweets = [??? for tweet in new_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los vectores correspondientes a cada nuevo tweet\n",
    "vectors_new_tweets = [??? for tweet in preprocess_new_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos la predicción sobre estos nuevos tweets\n",
    "pred_new_tweets = gb_clf.predict(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos el resultado para cada tweet\n",
    "for tweet, pred in zip(new_tweets, pred_new_tweets):\n",
    "    print(tweet, '--->', pred.upper())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('trustaware')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "415bf093bb7c32384f83c305df05bcae34dbf408c68dbcd8cd24937de3807095"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
